{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will be working with the Perceptron model using the scikit-learn library \n",
    "#to classify classes i.e. \"versicolor\" and \"virginica\"  the Iris dataset\n",
    "#which are based on two major features i.e. sepal length and petal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import a function for plotting decision boudaries\n",
    "# !pip install mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris dataset into a pandas DataFrame object\n",
    "df = pd.read_csv(\"./iris_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need only versicolor and virginica which are the first 100 rows\n",
    "\n",
    "df = df.iloc[50:150]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"sepal_length\", \"petal_length\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the labels in y\n",
    "y = df['species']\n",
    "\n",
    "# We can leave the original labels and use sklearn perceptron,\n",
    "# but to use mlxtend for plotting we need to encode the labels\n",
    "# versicolor = -1, virginica = 1\n",
    "\n",
    "y = np.where(y == 'versicolor', -1, 1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate one instance of the Perceptron class\n",
    "clf = Perceptron()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predict labels on X\n",
    "y_pred = clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual labels\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare actual and predicted labels\n",
    "print(y == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_   # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_  # bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "plot_decision_regions(X.to_numpy(), y, clf = clf)\n",
    "plt.title(\"Sklearn Perceptron\", fontsize = 18)\n",
    "plt.xlabel(\"sepal length\", fontsize = 15)\n",
    "plt.ylabel(\"petal length\", fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this classifier to predict the species of a flower with measurements 6 (sepal length) and 6 (petal length.)\n",
    "# Obviously from the graph, the label should be 1. \n",
    "\n",
    "clf.predict([[6, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy of this model for this particular data set\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is for plotting the error graph for this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"sepal_length\", \"petal_length\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPerceptron(object):\n",
    "    def __init__(self, eta = 0.5, epochs = 50):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.w1 = np.random.rand(1)\n",
    "        self.w2 = np.random.rand(1)\n",
    "        self.b = np.random.rand(1)\n",
    "        \n",
    "        self.errors = []\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                update = self.eta * (self.predict(xi) - yi)\n",
    "                self.w1 = self.w1 - update*xi[0]\n",
    "                self.w2 = self.w2 - update*xi[1]\n",
    "                self.b = self.b - update\n",
    "                errors = errors + int(update != 0)\n",
    "            if errors == 0:\n",
    "                return self\n",
    "            else:\n",
    "                self.errors.append(errors)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def weighted_sum(self, x):\n",
    "        self.w = np.array([self.w1, self.w2])\n",
    "        return np.dot(x, self.w) + self.b\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.where(self.weighted_sum(x) > 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate one instance of My_Perceptron class\n",
    "my_clf = MyPerceptron()\n",
    "\n",
    "# Call the fit method \n",
    "my_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on X\n",
    "y_pred = my_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the errors for each iteration\n",
    "# We also see the number of iterations needed for the algorithm to find the classifier\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(range(1, len(my_clf.errors)+1), my_clf.errors, \n",
    "         marker = \"o\")\n",
    "plt.title(\"Error plot\", fontsize = 15)\n",
    "plt.xlabel(\"The number of iterations\", fontsize = 15)\n",
    "plt.ylabel(\"The number of misclassifications\", fontsize = 15)\n",
    "plt.xticks(range(1, len(my_clf.errors)+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.w #weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.b #bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
